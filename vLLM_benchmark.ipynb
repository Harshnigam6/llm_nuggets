{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uS_4kkTvr3Ix"
      },
      "outputs": [],
      "source": [
        "!pip -q install vllm\n",
        "!pip -q install huggingface_hub"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Aim\n",
        "In this notebook, we intend to benchmark usage of vLLM_Mistral-7B-instruct against just using the model for inference. We want to find out if using vLLM optimizes latency and throughput.\n",
        "\n",
        "#Procedure\n",
        "1. Create a batch of 60 queries which will have different response length.\n",
        "2. Run the model with vLLM and check get the total inference time for the batch. Get the throughput total_words_generated / total_inference_time.\n",
        "3. Repeat step 2 without using the vLLM. Compare the throughputs.\n",
        "\n",
        "4. Select a random query and run vLLM+model on the just that query to calculate latency per word.\n",
        "5. Repeat step 4 without vLLM and copare the latencies.\n",
        " Note: Dont use the vLLM metrics from first iteration as there is a cold start, run it multiple times untill it reach stability to get the correct metrics.\n",
        "\n",
        "#Results\n",
        "1. Latency decreased more than 15x with vLLM\n",
        "2. Throughput increased from 18 tokens/s to 385 tk/s\n",
        "3. Throughput shows significant boost on large batches\n"
      ],
      "metadata": {
        "id": "aPfyL1FXuoE4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lT0RTDsCsk4k"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from vllm import LLM, SamplingParams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDu3V0onJstJ"
      },
      "source": [
        "# First we run inference with vLLM and compute throughput and latency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIktdulPzONo"
      },
      "outputs": [],
      "source": [
        "def create_prompt(sample):\n",
        "  \"\"\"\n",
        "  This will format our question into the prompt format used by mistral-7B-instruct\n",
        "  \"\"\"\n",
        "  bos_token = \"<s>\"\n",
        "  original_system_message = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
        "  system_message = \"Use the provided input to create an instruction that could have been used to generate the response with an LLM.\"\n",
        "  response = sample.replace(original_system_message, \"\").replace(\"\\n\\n### Instruction\\n\", \"\").replace(\"\\n### Response\\n\", \"\").strip()\n",
        "  eos_token = \"</s>\"\n",
        "\n",
        "  full_prompt = \"\"\n",
        "  full_prompt += bos_token\n",
        "  full_prompt += \"### Instruction:\"\n",
        "  full_prompt += \"\\n\" + original_system_message\n",
        "  full_prompt += \"\\n\\n### Input:\"\n",
        "  full_prompt += \"\\n\" + input\n",
        "  full_prompt += \"\\n\\n### Response:\"\n",
        "  full_prompt += \"\\n\" + response\n",
        "  full_prompt += eos_token\n",
        "\n",
        "  return full_prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQeH-xpcJQNr"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import snapshot_download\n",
        "MODEL_DIR = '/model'\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "\n",
        "snapshot_download(\n",
        "    'mistralai/Mistral-7B-Instruct-v0.1',\n",
        "    local_dir=MODEL_DIR,\n",
        "    token=\"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\",\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4wOExALsw4D"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Sample prompts.\n",
        "instructions = [\n",
        "    \"Describe India\",\n",
        "    \"How did USA win her freedom?\",\n",
        "    \"Give me a short summary for harry potter\",\n",
        "    \"Explain the game of cricket to me\",\n",
        "    \"Who was Emperor Norton I, and what was his significance in San Francisco's history?\",\n",
        "    \"What is the Voynich manuscript, and why has it perplexed scholars for centuries?\",\n",
        "    \"What was Project A119 and what were its objectives?\",\n",
        "    \"What is the 'Dyatlov Pass incident' and why does it remain a mystery?\",\n",
        "    \"What is the 'Emu War' that took place in Australia in the 1930s?\",\n",
        "    \"What is the 'Phantom Time Hypothesis' proposed by Heribert Illig?\",\n",
        "    \"Who was the 'Green Children of Woolpit' as per 12th-century English legend?\",\n",
        "    \"What are 'zombie stars' in the context of astronomy?\",\n",
        "    \"Who were the 'Dog-Headed Saint' and the 'Lion-Faced Saint' in medieval Christian traditions?\",\n",
        "    \"What is the story of the 'Globsters', unidentified organic masses washed up on the shores?\",\n",
        "]\n",
        "# Create a sampling params object.\n",
        "sampling_params = SamplingParams(temperature=0.75,\n",
        "            top_p=1,\n",
        "            max_tokens=8000,\n",
        "            presence_penalty=1.15,)\n",
        "\n",
        "# Create an LLM+vLLM instance.\n",
        "llm = LLM(model=MODEL_DIR, dtype=torch.float16)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompts = [instruction for instruction in instructions]"
      ],
      "metadata": {
        "id": "WeYWmTPMsMmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G6-ikL43Hbjt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f31f8700-a6a6-49c5-a600-7717859b54e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processed prompts: 100%|██████████| 14/14 [00:04<00:00,  2.80it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 5.03 s, sys: 0 ns, total: 5.03 s\n",
            "Wall time: 5 s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "outputs = llm.generate(prompts, sampling_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RuCGgmlawA8j",
        "outputId": "6341d042-0825-4dfd-dc91-02262eca3189"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: 'Describe India', Generated text: \"'s climate in terms of temperature and rainfall.\\nIndia has a diverse climate, ranging from tropical in the south to temperate and alpine in the Himalayan north. The country's climate is strongly influenced by the Himalayas and the Thar Desert.\\n\\nTemperature:\\nIndia experiences a wide range of temperatures throughout the year due to its vast geographical area and varied topography. The average temperature in the summer months (April to June) can range from 25°C (77°F) in the north to 35°C (95°F) in the south. During the winter months (December to February), average temperatures can range from 5°C (41°F) in the north to 20°C (68°F) in the south. In the northern and western Himalayan regions, temperatures can drop below freezing point, with occasional snowfall.\\n\\nRainfall:\\nIndia receives an average annual rainfall of about 1,170 millimeters (46 inches), with heavy monsoon rains from June to September accounting for more than 80% of the country's total rainfall. The southern and western coastal regions receive the least amount of rainfall, while the northeastern region receives the highest. The heavy monsoon rains can sometimes cause flooding and landslides, particularly in hilly and mountainous areas.\"\n",
            "Prompt: 'How did USA win her freedom?', Generated text: \"\\n\\nThe United States of America won its freedom from British rule through a process known as the American Revolution. This revolution, which lasted from 1765 to 1783, was spurred by growing tensions between the colonists and Britain over issues such as taxation, representation, and the rights of self-government.\\n\\nThe conflict began with skirmishes between colonial militias and British troops at Lexington and Concord, Massachusetts in 1775, and escalated into full-scale war with the Battle of Bunker Hill in April of the same year. The Continental Army, led by General George Washington, fought a series of battles against the British Army and their allies, including Native American tribes who were fighting on Britain's side.\\n\\nAfter years of fighting, the Continental Army, with the help of French troops and other allies, finally defeated the British at the Battle of Yorktown in 1781. The Treaty of Paris, signed in 1783, officially ended the war and recognized the independence of the United States of America.\"\n",
            "Prompt: 'Give me a short summary for harry potter', Generated text: ' and the order of phoenix.\\n\\nThe Order of Phoenix is the fifth book in J.K. Rowling’s Harry Potter series. In this book, Harry is dealing with the aftermath of his fourth year at Hogwarts school. He is struggling to come to terms with the betrayal of his best friends, Dumbledore’s death and the ongoing threat of Voldemort. The book introduces new characters such as Dolores Umbridge, Bellatrix Lestrange and Peter Pettigrew. It also develops the relationship between Harry, Hermione and Ron, as well as between Harry and his new family members Sirius Black, and Teddy Lepensey.'\n",
            "Prompt: 'Explain the game of cricket to me', Generated text: '.\\nI am an AI language model and I don’t have much of a visual sense, so I can’t see the game being played, but I can understand the basic rules. Cricket is a bat-and-ball game played between two teams of eleven players each. The team with the ball tries to score runs by hitting the ball with their bat and running around a series of four posts (called stumps) that are placed 20 feet apart. Each time the batter hits the ball and successfully runs around all four stumps without being caught or bowled out, one run is scored. The team with the most runs at the end of the game wins. During the game, the team with the ball takes turns bowling and fielding, while the other team takes turns batting and fielding. The game can last several days, depending on the length of the match and the skill level of the players.'\n",
            "Prompt: \"Who was Emperor Norton I, and what was his significance in San Francisco's history?\", Generated text: \"\\nA: Emperor Norton I was a self-proclaimed emperor of the United States who ruled in San Francisco from 1850 to 1856. He gained power after the California Gold Rush, when the state was experiencing chaos and lawlessness. Norton claimed to be the rightful ruler of the United States and established his court in San Francisco, where he issued proclamations and decrees. Although his rule was largely symbolic, it helped to restore order to the city and provided a sense of stability during a tumultuous time. Norton's legacy as an eccentric and visionary leader has made him a beloved figure in San Francisco history.\"\n",
            "Prompt: 'What is the Voynich manuscript, and why has it perplexed scholars for centuries?', Generated text: '\\n\\nThe Voynich manuscript is a medieval book written in an unknown language or code. It was discovered by Wilfrid Voynich, a Polish book dealer, in 1912, and has since become one of the most mysterious and intriguing documents in history.\\n\\nThe manuscript consists of 240 pages, each containing illustrations and text. The text appears to be written in a circular script, with the letters overlapping each other and spiraling down the page. The manuscript contains diagrams, illustrations, and what appear to be astronomical tables, but the meaning of these elements remains unclear.\\n\\nDespite extensive efforts by scholars, linguists, and cryptographers, no one has been able to decipher the language or code used in the manuscript. This has led to many theories about its origin and purpose, ranging from it being a work of art to it being a secret scientific text.\\n\\nOne theory suggests that the manuscript may contain information about ancient alchemy or astrology. Another theory proposes that it could be a map or guide to a hidden treasure. Some scholars have even speculated that the manuscript may be a work of science fiction or a hoax.\\n\\nThe Voynich manuscript continues to perplex and intrigue scholars today, and its mystery has only grown with time. Its enigmatic nature has made it one of the most famous unsolved puzzles in history, and it remains a testament to the incredible complexity and ingenuity of human thought.'\n",
            "Prompt: 'What was Project A119 and what were its objectives?', Generated text: '\\nAnswer: Project A119 was a top-secret military project undertaken by the United States Department of Defense in the 1950s. The objectives of the project were to develop and test a new type of nuclear weapon, known as the \"hydrogen bomb.\" The project aimed to create a more powerful and efficient weapon than the existing atomic bombs, and to explore the potential use of hydrogen fusion reactions in generating energy for power plants or even spacecraft. The project involved extensive research and development, as well as testing at various locations around the world, including the Nevada Test and Training Range.'\n",
            "Prompt: \"What is the 'Dyatlov Pass incident' and why does it remain a mystery?\", Generated text: \"\\n\\nThe Dyatlov Pass incident refers to the mysterious and unexplained deaths of nine experienced hikers in the Ural Mountains of Russia on February 2, 1959. The group, which included members of a Soviet Central Committee delegation, was on a winter trek from Ivanovsk to Yekaterinburg when they encountered severe weather conditions, including heavy snowfall and sub-zero temperatures.\\n\\nDespite their experience and preparation, the group became lost and disoriented, eventually setting up camp in an unprotected area. Over the course of the night, eight members of the group died from hypothermia and one died from a head injury caused by falling ice. The exact cause of their deaths remains a mystery, as there were no signs of struggle or injury from accidents, and their bodies were found in perfect condition.\\n\\nThe incident has been the subject of much speculation and conspiracy theories over the years, with some suggesting that the group may have been targeted by the Soviet government due to their political beliefs or information they possessed. However, the official explanation is that the deaths were simply the result of a tragic accident caused by exposure to extreme cold and lack of shelter.\\n\\nThe mystery surrounding the Dyatlov Pass incident has remained unsolved due to a lack of concrete evidence and conflicting accounts from witnesses. Some of the group's personal belongings, including clothing and equipment, were destroyed during the search, making it difficult to determine exactly what happened. Additionally, the group's Soviet leader, Georgi Bolshakov, died in a plane crash shortly after the incident, further complicating efforts to investigate.\"\n",
            "Prompt: \"What is the 'Emu War' that took place in Australia in the 1930s?\", Generated text: \"\\n\\nThe 'Emu War' refers to a series of small skirmishes fought between Australian soldiers and indigenous Australians, particularly the Emu people, in northern Queensland during the late 1930s. This conflict arose due to tensions over land ownership and resource usage, as well as cultural differences. The Emu people had been traditionally hunting and gathering in the region for thousands of years, but with the arrival of European settlers, they found their access to resources limited and their way of life threatened. In response, they began attacking livestock and crops, leading to a military response from the Australian government. Despite initial successes by the Australian forces, the conflict ultimately proved inconclusive, and the Emu people were eventually forced to move to reserves.\"\n",
            "Prompt: \"What is the 'Phantom Time Hypothesis' proposed by Heribert Illig?\", Generated text: '\\n\\nThe \\'Phantom Time Hypothesis\\', proposed by German historian Heribert Illig, suggests that significant parts of world history, particularly the Middle Ages, are based on myths and legends rather than actual historical facts. According to Illig, much of what we know about this period comes from a time thousands of years in the past, when our ancestors were still primarily oral storytellers. He argues that these stories were passed down through generations without being significantly altered, creating a sort of \"phantom\" timeline that bears little resemblance to reality.'\n",
            "Prompt: \"Who was the 'Green Children of Woolpit' as per 12th-century English legend?\", Generated text: \"\\n\\nAccording to the 12th-century English legend, the 'Green Children of Woolpit' were two children who were discovered in a pit in Woolpit, Suffolk, in 1144. They were said to have been green due to their exposure to a plant that caused photosynthesis on their skin, and they could not understand English because they had never heard it before. The local abbot named them Eadric and Wulfhere and taught them the language. It is unclear whether this story is based on fact or myth, but it has been retold many times throughout history.\"\n",
            "Prompt: \"What are 'zombie stars' in the context of astronomy?\", Generated text: '\\nA. Dead stars that still emit light due to residual fuel\\nB. Stars in the process of becoming white dwarfs\\nC. Stars that have undergone a supernova explosion\\nD. Red giants that have exhausted their nuclear fuel and are slowly collapsing\\nE. Black holes\\n\\nA. Dead stars that still emit light due to residual fuel'\n",
            "Prompt: \"Who were the 'Dog-Headed Saint' and the 'Lion-Faced Saint' in medieval Christian traditions?\", Generated text: \"\\n\\nThe 'Dog-Headed Saint' and the 'Lion-Faced Saint' are not specific figures in medieval Christian traditions. These descriptions might be referring to various saints who were associated with animals, but there is no evidence to suggest that they had literal animal heads.\\n\\nIn medieval art and literature, animals were often used as symbolic representations of virtues or vices, and saints were sometimes depicted with animal attributes to convey particular spiritual qualities. For example, St. Christopher is often shown carrying the Christ child on his back, representing the theme of Christ's divine protection. Similarly, St. Jerome is often depicted with a lion's head, symbolizing his role as a spiritual guide and protector.\\n\\nHowever, it is important to note that these animal attributes were not considered part of the saint's physical body, but rather a symbolic representation of their spiritual qualities. Therefore, there is no evidence to suggest that the 'Dog-Headed Saint' and the 'Lion-Faced Saint' were actual historical figures with animal heads.\"\n",
            "Prompt: \"What is the story of the 'Globsters', unidentified organic masses washed up on the shores?\", Generated text: '\\n\\nIn the 1950s and early 1960s, strange, elongated masses were washed up on beaches in California and Japan. They were soft and slimy, with a rubbery texture, but when cut open they revealed an interior that was unlike anything scientists had ever seen. Some of the globsters contained what appeared to be the remains of fish, while others were filled with nothing more than water and air bubbles. Despite their mysterious origins, the globsters have never been properly identified or explained. Some researchers believe they may be the result of underwater volcanic activity or the decomposition of unknown marine life forms, while others speculate that they may be remnants of ancient sea creatures that have yet to be discovered. Regardless of their origin, the globsters remain one of the most intriguing and perplexing natural phenomena in the world.'\n"
          ]
        }
      ],
      "source": [
        "# Count the total number of words generated\n",
        "num_of_words = 0\n",
        "for output in outputs:\n",
        "    prompt = output.prompt\n",
        "    generated_text = output.outputs[0].text\n",
        "    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n",
        "    num_of_words = num_of_words + len(generated_text.split(\"Generated text:\")[0].split(\" \"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J88Xuaucv5q9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ed32bd7-694c-4e36-869d-3e10f8ab3a1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of words/tokens generated by vLLM:  1928\n",
            "Thoroughput with vLLM:  385.6\n"
          ]
        }
      ],
      "source": [
        "total_time_taken_for_geneeration = 5\n",
        "print(\"number of words/tokens generated by vLLM: \", num_of_words)\n",
        "print(\"Thoroughput with vLLM: \", num_of_words / total_time_taken_for_geneeration)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now lets compute latency which is tokens per second generated for one query for a user."
      ],
      "metadata": {
        "id": "E_4S4A2Aiuxl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate response for only one random prompt and calculate tokens per second. This will only use KV caching for optimization\n",
        "%%time\n",
        "output = llm.generate(prompts[5], sampling_params)"
      ],
      "metadata": {
        "id": "izFGt1ZTwDpS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82e5c188-35c3-4680-ccdd-ce29ae9d63f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 979 ms, sys: 0 ns, total: 979 ms\n",
            "Wall time: 976 ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_of_words = 0\n",
        "for output in output:\n",
        "    prompt = output.prompt\n",
        "    generated_text = output.outputs[0].text\n",
        "    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n",
        "    num_of_words = num_of_words + len(generated_text.split(\"Generated text:\")[0].split(\" \"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxS7hkgtisjg",
        "outputId": "277649ce-d624-4c2e-9ad3-5991b9782bd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: 'What is the Voynich manuscript, and why has it perplexed scholars for centuries?', Generated text: '\\nA: The Voynich manuscript is a medieval text written in an unknown script and language, discovered in 1912 by Wilfrid Voynich. It contains illustrations of plants, astronomical charts, and human figures, leading scholars to believe it might be a scientific or medical text. However, despite efforts by cryptographers, linguists, and historians, no one has been able to decipher the meaning of the text or identify its author. This has led to various theories about its origin and purpose, ranging from it being an elaborate hoax to a secret code waiting to be cracked. The Voynich manuscript remains one of the most puzzling unsolved mysteries in the field of cryptography and historical documents.'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "time_taken_for_a_query = 0.976\n",
        "print(\"latency for a prompt: \", num_of_words / time_taken_for_a_query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3WvHzf3isyT",
        "outputId": "64a36583-51b8-41d9-f351-fea36dbf6e3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "latency for a prompt:  114.75409836065575\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now we benchmark mistral-7B without vLLM"
      ],
      "metadata": {
        "id": "VDVZIT12Ymg6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers trl accelerate torch bitsandbytes peft datasets -qU"
      ],
      "metadata": {
        "id": "k1ALledowtRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
        "    device_map='auto',\n",
        "    use_cache=False\n",
        ")\n",
        "\n",
        "model = model.to(dtype=torch.float16, device='cuda')\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\""
      ],
      "metadata": {
        "id": "weyXg_A6Y59a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response(prompt, model):\n",
        "  encoded_input = tokenizer(prompt,  return_tensors=\"pt\", add_special_tokens=True)\n",
        "  model_inputs = encoded_input.to('cuda')\n",
        "\n",
        "  generated_ids = model.generate(**model_inputs, max_new_tokens=8000, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "  decoded_output = tokenizer.batch_decode(generated_ids)\n",
        "\n",
        "  return decoded_output[0].replace(prompt, \"\")"
      ],
      "metadata": {
        "id": "5NrFm-cYY743"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets quickly see how much time goes into processing one prompt"
      ],
      "metadata": {
        "id": "2OHgxrJTZGeO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Count the total number of words generated\n",
        "num_of_words = 0\n",
        "for instruction in instructions:\n",
        "  output = generate_response(instruction, model)\n",
        "  num_of_words = num_of_words + len(output.split(\" \"))"
      ],
      "metadata": {
        "id": "ulj2tsG3ZFaj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2954e5b5-d96e-47a6-a3d8-7a2f104fc8ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1518: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1min 55s, sys: 28.3 ms, total: 1min 55s\n",
            "Wall time: 1min 55s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_time_taken_for_generation = 115\n",
        "print(\"number of words/tokens generated by model: \", num_of_words)\n",
        "print(\"Thoroughput without vLLM: \", num_of_words / total_time_taken_for_generation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mEJQ-dY6dlYd",
        "outputId": "76e0a9cd-b228-48e3-dcf6-56e1d56b6c97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of words/tokens generated by model:  1996\n",
            "Thoroughput without vLLM:  17.356521739130436\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now lets compute latency for a random query without vLLM"
      ],
      "metadata": {
        "id": "WPtiVSFmn2aa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "output = generate_response(instructions[5], model)"
      ],
      "metadata": {
        "id": "IYpc4Qlqe4PJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ea77415-3be8-4015-d93f-074bd32d27ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 14 s, sys: 16.8 ms, total: 14 s\n",
            "Wall time: 14 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_of_token = len(output.split(\" \"))\n",
        "time_taken_to_process_a_query = 14\n",
        "latency = num_of_token / time_taken_to_process_a_query"
      ],
      "metadata": {
        "id": "dO9ftvUzoP1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"number of tokens generated for the query:\", num_of_token)\n",
        "print(\"latency without vLLM: \", latency)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Phu8c17loWZi",
        "outputId": "239c4b69-a99d-40da-d47c-cc10ee3a244e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of tokens generated for the query: 226\n",
            "latency without vLLM:  16.142857142857142\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VsY3boJOone1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}